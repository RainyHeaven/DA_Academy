{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[문제181] 이 주소로 접속하셔서 게시글을 출력하세요.   \n",
    "http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\n",
    "```python\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106'\n",
    "html = req.urlopen(url)\n",
    "soup = bs(html, 'html.parser')\n",
    "articles = soup.findAll('p', class_='con')\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    print('{}번째 댓글: {}'.format(i+1, article.string.strip()))\n",
    "```    \n",
    "```python\n",
    "# 선생님답\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "result = soup.find_all('p', class_=\"con\")\n",
    "for i in result:\n",
    "    print(i.get_text(strip=True))\n",
    "\n",
    "\n",
    "\n",
    "url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\n",
    "res = req.urlopen(url).read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(res,\"html.parser\")\n",
    "result = soup.find_all('p', class_=\"con\")\n",
    "for i in result:\n",
    "    print(i.get_text(strip=True))\n",
    "```   \n",
    "\n",
    "[문제182] 게시글 뿐만 아니라 게시날짜 정보도 같이 출력하시오 !\n",
    "```\n",
    "2017.04.12 19:48 레이디버그 3기나오면 좋은사람 손~~\n",
    "```\n",
    "```python\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106'\n",
    "html = req.urlopen(url)\n",
    "soup = bs(html, 'html.parser')\n",
    "articles = soup.findAll('li', class_='spot_')\n",
    "\n",
    "for article in articles:\n",
    "    date = article.find('span', class_='date').string\n",
    "    reply = article.find('p', class_='con').string.strip()\n",
    "    print(date, reply)\n",
    "```    \n",
    "```python\n",
    "# 선생님 답\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?hmpMnuId=106\"\n",
    "res = req.urlopen(url)\n",
    "soup= BeautifulSoup(res, \"html.parser\")\n",
    "a = soup.find_all('p',class_=\"con\")\n",
    "b = soup.find_all('span',class_=\"date\")\n",
    "print(b[0].get_text())\n",
    "print(b[0].text)\n",
    "\n",
    "cnt= 0\n",
    "for i in a:\n",
    "    print(b[cnt].text,i.get_text(strip=True))\n",
    "    cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "```\n",
    "\n",
    "[문제183] 게시판에 게시글 전부를 수집해주세요.\n",
    "```python\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "result = []\n",
    "for i in range(1, 17):\n",
    "    url = 'http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page={}&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&'.format(i)\n",
    "    html = req.urlopen(url)\n",
    "    soup = bs(html, 'html.parser')\n",
    "    articles = soup.findAll('li', class_='spot_')\n",
    "    for article in articles:\n",
    "        date = article.find('span', class_='date').string\n",
    "        reply = article.find('p', class_='con').string.strip()\n",
    "        result.append([str(date), str(reply)])\n",
    "```        \n",
    "```python\n",
    "# 선생님 답\n",
    "for i in range(1,16):\n",
    "    url = \"http://home.ebs.co.kr/ladybug/board/6/10059819/oneBoardList?c.page=\" +str(i)+ \"&hmpMnuId=106&searchKeywordValue=0&bbsId=10059819&searchKeyword=&searchCondition=&searchConditionValue=0&\"\n",
    "\n",
    "    res = req.urlopen(url)\n",
    "    soup= BeautifulSoup(res, \"html.parser\")\n",
    "    a = soup.find_all('p',class_=\"con\")\n",
    "    b = soup.find_all('span',class_=\"date\")\n",
    "    cnt= 0\n",
    "    content =[]\n",
    "    for i in a:\n",
    "        content.append(b[cnt].text + i.get_text(strip=True))\n",
    "        cnt += 1\n",
    "    \n",
    "    for j in content:\n",
    "        print(j)\n",
    "```\n",
    "\n",
    "[문제184] 동아일보에서 인공지능에 기사 스크롤링 해주세요.\n",
    "```python\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "result = []\n",
    "for i in range(1, 3):\n",
    "    main_url = 'http://news.donga.com/search?p={}&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'.format(i)\n",
    "    main_html = req.urlopen(main_url)\n",
    "    main_soup = bs(main_html, 'html.parser')\n",
    "    links = main_soup.findAll('p', class_='tit')\n",
    "    for link in links:\n",
    "        article_html = req.urlopen(link.a.attrs['href'])\n",
    "        article_soup = bs(article_html, 'html.parser')\n",
    "        article = article_soup.find('div', class_='article_txt').text\n",
    "        result.append(article.split('@donga.com')[0])\n",
    "```        \n",
    "```python\n",
    "#선생님답\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "params = []\n",
    "\n",
    "for i in range(1,20,15):\n",
    "    list_url = \"http://news.donga.com/search?p=\"+str(i)+\"&query=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1\"\n",
    "\n",
    "    url = urllib.request.Request(list_url)\n",
    "    res = urllib.request.urlopen(url, timeout=100).read().decode(\"utf-8\")\n",
    "    soup= BeautifulSoup(res, \"html.parser\")\n",
    "    for link in soup.findAll('p', {'class':'tit'}):\n",
    "        params.append(link.find('a').get('href'))\n",
    "\n",
    "print(params)\n",
    "\n",
    "\n",
    "cn = 0\n",
    "txt= []\n",
    "for i in params:\n",
    "    print(i)\n",
    "    url = urllib.request.Request(i)\n",
    "    res = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "    soup= BeautifulSoup(res, \"html.parser\")\n",
    "    result = soup.find_all('div',class_='article_txt')\n",
    "    \n",
    "    \n",
    "    for i in result:\n",
    "        #print(i.text)\n",
    "       txt.append(i.text)\n",
    "\n",
    "txt[0]\n",
    "\n",
    "txt[0][0:txt[0].find('Copyright')]\n",
    "\n",
    "for i in range(0,30):\n",
    "    print(txt[i][0:txt[i].find('Copyright')])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.request\n",
    "-  HTML문서를 가져오기 위한 함수를 제공하는 패키지\n",
    "-  ### urlopen()\n",
    "    -  해당 주소에 접속하여 HTML 문서를 가져온다\n",
    "    -  `urllib.request.urlopen(url주소)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beautifulsoup\n",
    "-  HTML 문서 조작에 필요한 함수를 제공하는 패키지\n",
    "-  HTML 문서를 다루기 위해 `Tag`, `NavigableString`, `BeautifulSoup`, `Comment` 객체를 사용\n",
    "    -  `Tag`\n",
    "        -  HTML문서의 태그에 대응하는 객체\n",
    "        -  attribute\n",
    "            -  `.name`: 해당 태그의 이름을 문자열로 리턴\n",
    "            -  `.attrs`: 해당 태그의 attribute 들을 딕셔너리 형태로 리턴\n",
    "            -  `.string`: 해당 태그가 1개의 string을 갖고 있다면 문자열을, string이 없거나 2개 이상이면 None을, 1개의 child tag가 있다면 child tag의 'string' attribute를 리턴\n",
    "            -  `.stripped_strings`: whitespace를 제거한 string\n",
    "            -  `.text`: 해당 태그의 모든 child string을 주어진 연결자로 연결하여 문자열로 리턴\n",
    "            -  `.get_text`: 해당 태그의 모든 문자열을 \n",
    "            -  `.contents`: 해당 태그에 포함된 모든 내용을 리스트로 리턴\n",
    "            -  `.children`: 해당 태그에 포함된 모든 하위 태그의 내용을 generator로 리턴\n",
    "            -  `.parents`: 해당 태그의 상위 레벨 태그를 리턴\n",
    "            -  `.next_sibling`: 해당 태그와 동일한 레벨에 있는 다음 태그를 리턴\n",
    "            -  `.previous_sibling`: 해당 태그와 동일한 레벨에 있는 이전 태그를 리턴\n",
    "            \n",
    "    -  `NavigableString` \n",
    "        -  HTML문서의 태그 안의 문자열과 대응하는 객체로 태그.string을 통해 생성\n",
    "        -  파이썬의 Unicode string과 같으나 추가적인 몇가지 기능을 갖고 있다\n",
    "    \n",
    "    -   `Comment`\n",
    "        -  `NavigableString`의 특별한 형태\n",
    "  \n",
    "    -  `beautifulsoup`\n",
    "        -  문서 전체와 대응하는 객체   \n",
    "<br></br>   \n",
    "-  ### BeautifulSoup\n",
    "    -  HTML문서를 파싱해 beautifulsoup 객체로 리턴\n",
    "    -  `BeautifulSoup(HTML문서, 사용할파서)`   \n",
    "<br></br>        \n",
    "-  ### find()\n",
    "    -  해당 태그에서 매칭되는 첫번째 태그 리턴\n",
    "    -  `find(매칭조건)`\n",
    "        -  조건\n",
    "            -  문자열\n",
    "            -  정규표현식(re 패키지 필요)\n",
    "            -  리스트: 리스트 내부의 조건 중 어떤것이라도 매칭\n",
    "            -  True: 문서의 모든 태그와 매칭\n",
    "            -  함수   \n",
    "<br></br>    \n",
    "-  ### find_all()\n",
    "    -  해당 태그에서 매칭되는 모든 태그 리턴\n",
    "    -  `find_all(매칭조건)`\n",
    "        -  조건\n",
    "            -  `name`: 태그의 이름(문자열, 정규표현식, 리스트, True, 함수)\n",
    "            -  `attribute명=attribute값`(문자열, 정규표현식, 리스트, True, 함수)    \n",
    "                *`attrs={attribute명:attribute값}`형태로도 사용 가능*\n",
    "            -  CSS class\n",
    "            -  `string`\n",
    "            -  `limit`: 지정한 갯수까지만 매칭\n",
    "    -  `태그(매칭조건)` 형태로도 사용 가능   \n",
    "<br></br>            \n",
    "-  ### findAll()\n",
    "    -  `find_all()`의 이전 이름(BS4에서 Python style guide에 맞춰 변경됨)   \n",
    "<br></br>    \n",
    "-  ### get_text()\n",
    "    -  문서나 태그에 포함된 문자열만 Unicode 문자열로 리턴\n",
    "    -  `태그.get_text(*연결자, *strip=공백제거여부)` *생략가능   \n",
    "<br></br>    \n",
    "-  ### get()\n",
    "    -  해당 태그에서 지정한 attribute의 값을 리턴\n",
    "    -  `태그.get(attribute명)`   \n",
    "<br></br>\n",
    "-  ### next_sibling()\n",
    "    -  해당 태그와 동일한 레벨에 있는 다음 태그를 리턴\n",
    "    -  `태그.next_sibling(name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1> 스크래핑 </h1>\n",
    "            <p> 웹페이지 분석하기 </p>\n",
    "            <p> 데이터 정제작업하기1 </p>\n",
    "            <p> 데이터 정제작업하기2 </p>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 스크래핑 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = soup.html.body.h1\n",
    "h1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 웹페이지 분석하기 '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = soup.html.body.p\n",
    "p1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = p1.next_sibling\n",
    "p2.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 데이터 정제작업하기1 '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = p1.next_sibling.next_sibling\n",
    "p2.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 데이터 정제작업하기2 '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3 = p1.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "p3.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id='title'> beautifulsoup </h1>\n",
    "            <p id='subtitle'> 스크래핑 </p>\n",
    "            <p> 데이터 추출하기 </p>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' beautifulsoup '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs(html, 'html.parser')\n",
    "soup.find(id='title').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' beautifulsoup '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find(id='title')\n",
    "title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 스크래핑 '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id='subtitle').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <ul> 스크래핑 </ul>\n",
    "            <li><a href='http://www.itwill.com'> 아이티윌 </a></li>\n",
    "            <li><a href='http://www.naver.com'> 네이버 </a></li>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-af60c177641e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mul\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "soup = bs(html, 'html.parser')\n",
    "a1 = soup.html.body.ul.li.a\n",
    "a1.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 아이티윌 '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 네이버 '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')[1].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': 'http://www.itwill.com'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = soup.a\n",
    "a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'href' in a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.itwill.com'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.itwill.com'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.itwill.com\n",
      " 아이티윌 \n",
      "http://www.naver.com\n",
      " 네이버 \n"
     ]
    }
   ],
   "source": [
    "link = soup.find_all('a')\n",
    "for i in link:\n",
    "    print(i.attrs['href'])\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 내 홈페이지 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('C:/Users/stu/git/DA_Academy/a.html', encoding='utf-8') as html:\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "soup.find('title').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<p align=\"center\"> 환영합니다. </p>\n",
       "<p align=\"left\"> 이름: 홍길동<br/> 나이: 25<br/>\n",
       "\t\t\t\t   취미: 음악감상 </p>\n",
       "<p align=\"right\"> 오늘 하루도 행복하세요... </p>\n",
       "<a class=\"cafe1\" href=\"http://itwill.co.kr\" id=\"link1\"> 아이티윌 </a>\n",
       "<a class=\"cafe2\" href=\"http://www.naver.co.kr\" id=\"link2\"> 네이버 </a>\n",
       "<a class=\"cafe3\" href=\"http://www.google.com\" id=\"link3\"> 구글 </a>\n",
       "</body>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p align=\"center\"> 환영합니다. </p>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 환영합니다. \n",
      "None\n",
      " 오늘 하루도 행복하세요... \n"
     ]
    }
   ],
   "source": [
    "p = soup.find_all('p')\n",
    "for i in p:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'method'>\n",
      "<bound method Tag.get_text of <p align=\"center\"> 환영합니다. </p>>\n",
      "<class 'method'>\n",
      "<bound method Tag.get_text of <p align=\"left\"> 이름: 홍길동<br/> 나이: 25<br/>\n",
      "\t\t\t\t   취미: 음악감상 </p>>\n",
      "<class 'method'>\n",
      "<bound method Tag.get_text of <p align=\"right\"> 오늘 하루도 행복하세요... </p>>\n"
     ]
    }
   ],
   "source": [
    "for i in p:\n",
    "    print(type(i.get_text))\n",
    "    print(i.get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 환영합니다. \n",
      " 이름: 홍길동 나이: 25\n",
      "\t\t\t\t   취미: 음악감상 \n",
      " 오늘 하루도 행복하세요... \n"
     ]
    }
   ],
   "source": [
    "for i in p:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 환영합니다. \n",
      " 이름: 홍길동 나이: 25\n",
      "\t\t\t\t   취미: 음악감상 \n",
      " 오늘 하루도 행복하세요... \n"
     ]
    }
   ],
   "source": [
    "p = soup.findAll('p')\n",
    "for i in p:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('body').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 환영합니다. \\n 이름: 홍길동 나이: 25\\n\\t\\t\\t\\t   취미: 음악감상 \\n 오늘 하루도 행복하세요... \\n 아이티윌 \\n 네이버 \\n 구글 \\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('body').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 환영합니다. \\n 이름: 홍길동 나이: 25\\n\\t\\t\\t\\t   취미: 음악감상 \\n 오늘 하루도 행복하세요... \\n 아이티윌 \\n 네이버 \\n 구글 \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'환영합니다.이름: 홍길동나이: 25취미: 음악감상오늘 하루도 행복하세요...아이티윌네이버구글'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('body').get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 환영합니다. \n",
      " 이름: 홍길동 나이: 25\n",
      "\t\t\t\t   취미: 음악감상 \n",
      " 오늘 하루도 행복하세요... \n",
      " 아이티윌 \n",
      " 네이버 \n",
      " 구글 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "body = soup.find_all('body')\n",
    "for i in body:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 환영합니다. \n",
      " 이름: 홍길동 나이: 25\n",
      "\t\t\t\t   취미: 음악감상 \n",
      " 오늘 하루도 행복하세요... \n",
      " 아이티윌 \n",
      " 네이버 \n",
      " 구글 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "body = soup.findAll('body')\n",
    "for i in body:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 아이티윌 \n",
      "\n",
      " 네이버 \n",
      "\n",
      " 구글 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = soup.findAll('a')\n",
    "for i in a:\n",
    "    print(i.get_text()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      " 아이티윌 \n",
      "\n",
      "http://www.naver.co.kr\n",
      " 네이버 \n",
      "\n",
      "http://www.google.com\n",
      " 구글 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "link = soup.find_all('a')\n",
    "for i in link:\n",
    "    print(i.attrs['href'])\n",
    "    print(i.string+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      " 아이티윌 \n",
      "\n",
      "http://www.naver.co.kr\n",
      " 네이버 \n",
      "\n",
      "http://www.google.com\n",
      " 구글 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "link = soup.findAll('a')\n",
    "for i in link:\n",
    "    print(i.attrs['href'])\n",
    "    print(i.get_text()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"cafe2\" href=\"http://www.naver.co.kr\" id=\"link2\"> 네이버 </a>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 아이티윌 \n"
     ]
    }
   ],
   "source": [
    "a1 = soup.findAll('a', {'class':'cafe1'})\n",
    "for i in a1:\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 아이티윌 \n"
     ]
    }
   ],
   "source": [
    "a1 = soup.find('a',{'class':'cafe1'})\n",
    "for i in a1:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.naver.co.kr\n",
      " 네이버 \n"
     ]
    }
   ],
   "source": [
    "a1 = soup.findAll('a',{'class':'cafe2'})\n",
    "for i in a1:\n",
    "    print(i.attrs['href'])\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      " 아이티윌 \n"
     ]
    }
   ],
   "source": [
    "a1 = soup.findAll('a', {'id':'link1'})\n",
    "for i in a1:\n",
    "    print(i.attrs['href'])\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      " 아이티윌 \n"
     ]
    }
   ],
   "source": [
    "for i in soup.findAll(class_ = 'cafe1'):\n",
    "    print(i.attrs['href'])\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      " 아이티윌 \n"
     ]
    }
   ],
   "source": [
    "for i in soup.findAll(id='link1'):\n",
    "    print(i.attrs['href'])\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://itwill.co.kr\n",
      "http://www.naver.co.kr\n",
      "http://www.google.com\n"
     ]
    }
   ],
   "source": [
    "for i in soup.findAll('a'):\n",
    "    print(i.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p align=\"center\"> 환영합니다. </p>, <p align=\"left\"> 이름: 홍길동<br/> 나이: 25<br/>\n",
       " \t\t\t\t   취미: 음악감상 </p>, <p align=\"right\"> 오늘 하루도 행복하세요... </p>, <a class=\"cafe1\" href=\"http://itwill.co.kr\" id=\"link1\"> 아이티윌 </a>, <a class=\"cafe2\" href=\"http://www.naver.co.kr\" id=\"link2\"> 네이버 </a>, <a class=\"cafe3\" href=\"http://www.google.com\" id=\"link3\"> 구글 </a>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll(['a','p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x558e390>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "url = 'http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "res = req.urlopen(url)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'기상청 육상 중기예보'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs(res, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "links = soup.findAll('a')\n",
    "print(len(links))\n",
    "for link in links:\n",
    "    print(link['href'])\n",
    "    print(link.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "이번 예보기간에는 고기압의 영향으로 대체로 맑은 날이 많겠습니다<br />기온은 평년(최저기온: 7~17℃, 최고기온: 21~25℃)과 비슷하겠으나, 전반에는 조금 낮겠습니다.<br />강수량은 평년(1~6mm)보다 적겠습니다.\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름많음\n",
      "구름많음\n",
      "구름많음\n",
      "구름많음\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n",
      "구름조금\n"
     ]
    }
   ],
   "source": [
    "wfs = soup.findAll('wf')\n",
    "print(len(wfs))\n",
    "for wf in wfs:\n",
    "    print(wf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이번 예보기간에는 고기압의 영향으로 대체로 맑은 날이 많겠습니다<br />기온은 평년(최저기온: 7~17℃, 최고기온: 21~25℃)과 비슷하겠으나, 전반에는 조금 낮겠습니다.<br />강수량은 평년(1~6mm)보다 적겠습니다.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf = soup.find('wf')\n",
    "wf.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "서울\n",
      "인천\n",
      "수원\n",
      "파주\n",
      "춘천\n",
      "원주\n",
      "강릉\n",
      "대전\n",
      "세종\n",
      "홍성\n",
      "청주\n",
      "광주\n",
      "목포\n",
      "여수\n",
      "전주\n",
      "군산\n",
      "부산\n",
      "울산\n",
      "창원\n",
      "대구\n",
      "안동\n",
      "포항\n",
      "제주\n",
      "서귀포\n"
     ]
    }
   ],
   "source": [
    "cities = soup.findAll('city')\n",
    "print(len(cities))\n",
    "for city in cities:\n",
    "    print(city.string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
