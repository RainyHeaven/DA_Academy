{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "문제 linear regression 학습을 통해서 입력값에 대한 예측값을 출력하세요\n",
    "x1 x2 x3 y\n",
    "73 80 75 152\n",
    "93 88 93 185\n",
    "89 91 90 180\n",
    "96 98 100 196\n",
    "73 66 70 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 275.17792 [-0.39429256] [-0.39429864] [-0.39423653] [-0.39910102]\n",
      "20 250.09073 [-0.296991] [-0.29712006] [-0.29581496] [-0.39796913]\n",
      "40 225.0037 [-0.1996884] [-0.19994321] [-0.19739339] [-0.39683723]\n",
      "60 199.91684 [-0.10238451] [-0.1027685] [-0.09897213] [-0.39570534]\n",
      "80 174.83026 [-0.00507908] [-0.00559655] [-0.00055119] [-0.39457345]\n",
      "100 149.74408 [0.09222829] [0.0915716] [0.0978692] [-0.39344156]\n",
      "120 124.65854 [0.1895381] [0.18873438] [0.19628863] [-0.39230967]\n",
      "140 99.574135 [0.28685108] [0.2858891] [0.29470628] [-0.39117754]\n",
      "160 74.492 [0.38416812] [0.3830298] [0.39312017] [-0.39004505]\n",
      "180 49.41559 [0.48148948] [0.4801415] [0.49152374] [-0.38891238]\n",
      "200 24.36228 [0.5788025] [0.5771617] [0.5898823] [-0.3877791]\n",
      "220 1.3147656 [0.66882366] [0.66588956] [0.680421] [-0.38672227]\n",
      "240 1.2844253 [0.67136455] [0.6631942] [0.6806864] [-0.3866492]\n",
      "260 1.2723868 [0.67386246] [0.66046834] [0.6809081] [-0.3865762]\n",
      "280 1.2603993 [0.6763564] [0.65775114] [0.6811254] [-0.38650286]\n",
      "300 1.2484832 [0.6788458] [0.6550428] [0.6813381] [-0.38642895]\n",
      "320 1.2366283 [0.6813311] [0.6523435] [0.68154615] [-0.38635457]\n",
      "340 1.2248406 [0.6838117] [0.6496535] [0.68174964] [-0.38627976]\n",
      "360 1.2131236 [0.68628776] [0.64697313] [0.6819483] [-0.38620454]\n",
      "380 1.20147 [0.6887589] [0.64430237] [0.6821421] [-0.38612884]\n",
      "400 1.1898825 [0.69122505] [0.6416415] [0.6823311] [-0.38605258]\n",
      "420 1.1783675 [0.6936863] [0.63899076] [0.6825148] [-0.38597587]\n",
      "440 1.1669283 [0.69614214] [0.6363506] [0.6826937] [-0.38589868]\n",
      "460 1.1555599 [0.69859236] [0.633721] [0.6828672] [-0.38582104]\n",
      "480 1.144268 [0.7010372] [0.6311024] [0.68303555] [-0.38574296]\n",
      "500 1.1330515 [0.70347625] [0.6284948] [0.68319875] [-0.38566428]\n",
      "520 1.1219251 [0.70590925] [0.6258989] [0.68335634] [-0.3855851]\n",
      "540 1.1108637 [0.70833623] [0.6233145] [0.6835086] [-0.3855054]\n",
      "560 1.0998929 [0.7107567] [0.62074214] [0.6836551] [-0.3854252]\n",
      "580 1.0890037 [0.71317077] [0.6181821] [0.683796] [-0.3853445]\n",
      "600 1.0782028 [0.71557814] [0.61563456] [0.68393123] [-0.3852633]\n",
      "620 1.067498 [0.7179787] [0.61309993] [0.68406034] [-0.38518155]\n",
      "640 1.0568652 [0.7203721] [0.6105782] [0.68418366] [-0.3850993]\n",
      "660 1.046334 [0.7227583] [0.60807014] [0.68430084] [-0.38501644]\n",
      "680 1.0358967 [0.72513676] [0.6055758] [0.6844118] [-0.38493305]\n",
      "700 1.0255442 [0.72750765] [0.6030955] [0.68451667] [-0.38484913]\n",
      "720 1.0152974 [0.7298705] [0.6006293] [0.68461525] [-0.38476467]\n",
      "740 1.0051402 [0.7322252] [0.598178] [0.6847075] [-0.38467965]\n",
      "760 0.9950873 [0.7345714] [0.5957418] [0.6847931] [-0.38459408]\n",
      "780 0.98513865 [0.7369088] [0.59332085] [0.68487215] [-0.38450795]\n",
      "800 0.9752992 [0.7392373] [0.5909158] [0.6849446] [-0.3844213]\n",
      "820 0.9655506 [0.74155664] [0.5885266] [0.68501025] [-0.38433403]\n",
      "840 0.9559202 [0.74386656] [0.58615386] [0.6850689] [-0.3842462]\n",
      "860 0.94639874 [0.7461668] [0.5837978] [0.6851207] [-0.38415784]\n",
      "880 0.93698555 [0.7484571] [0.581459] [0.6851656] [-0.3840689]\n",
      "900 0.9276824 [0.7507372] [0.5791375] [0.6852031] [-0.3839794]\n",
      "920 0.918497 [0.7530067] [0.576834] [0.6852334] [-0.38388935]\n",
      "940 0.90943456 [0.7552653] [0.5745488] [0.6852564] [-0.38379872]\n",
      "960 0.9004762 [0.7575128] [0.5722821] [0.68527216] [-0.3837075]\n",
      "980 0.89165133 [0.7597491] [0.5700342] [0.6852803] [-0.38361567]\n",
      "1000 0.88293815 [0.7619736] [0.56780565] [0.685281] [-0.3835233]\n",
      "1020 0.87435013 [0.7641864] [0.5655967] [0.685274] [-0.3834303]\n",
      "1040 0.8658886 [0.76638687] [0.5634079] [0.6852592] [-0.38333675]\n",
      "1060 0.85755605 [0.7685748] [0.56123936] [0.68523663] [-0.38324264]\n",
      "1080 0.8493417 [0.77074987] [0.55909187] [0.6852063] [-0.38314798]\n",
      "1100 0.84126604 [0.7729118] [0.5569654] [0.685168] [-0.38305274]\n",
      "1120 0.83331335 [0.77506053] [0.5548606] [0.6851217] [-0.38295692]\n",
      "1140 0.8254901 [0.77719533] [0.55277735] [0.6850672] [-0.38286054]\n",
      "1160 0.8178052 [0.7793163] [0.5507165] [0.6850048] [-0.3827636]\n",
      "1180 0.8102535 [0.781423] [0.5486783] [0.6849342] [-0.38266608]\n",
      "1200 0.8028268 [0.78351486] [0.54666287] [0.6848552] [-0.38256806]\n",
      "1220 0.79554504 [0.78559184] [0.544671] [0.68476814] [-0.38246948]\n",
      "1240 0.7883931 [0.7876536] [0.5427026] [0.68467265] [-0.38237035]\n",
      "1260 0.7813792 [0.7897002] [0.54075813] [0.6845688] [-0.3822707]\n",
      "1280 0.7745008 [0.7917308] [0.53883785] [0.6844565] [-0.38217053]\n",
      "1300 0.76776224 [0.7937454] [0.5369423] [0.68433595] [-0.3820698]\n",
      "1320 0.76115495 [0.7957438] [0.5350716] [0.68420684] [-0.3819685]\n",
      "1340 0.75469476 [0.79772544] [0.53322595] [0.68406916] [-0.38186672]\n",
      "1360 0.7483758 [0.7996904] [0.53140575] [0.6839232] [-0.38176447]\n",
      "1380 0.7421829 [0.80163836] [0.52961135] [0.68376863] [-0.38166174]\n",
      "1400 0.7361271 [0.8035689] [0.5278427] [0.6836056] [-0.38155857]\n",
      "1420 0.7302199 [0.80548185] [0.5261003] [0.68343395] [-0.38145486]\n",
      "1440 0.7244411 [0.80737716] [0.52438426] [0.68325394] [-0.38135064]\n",
      "1460 0.7187986 [0.80925435] [0.52269495] [0.68306535] [-0.381246]\n",
      "1480 0.7132933 [0.8111134] [0.52103215] [0.6828682] [-0.381141]\n",
      "1500 0.70792556 [0.8129542] [0.5193966] [0.682663] [-0.3810355]\n",
      "1520 0.7026868 [0.81477624] [0.517788] [0.6824492] [-0.3809295]\n",
      "1540 0.6975804 [0.8165796] [0.5162065] [0.68222696] [-0.38082314]\n",
      "1560 0.6926053 [0.81836396] [0.5146525] [0.68199646] [-0.38071644]\n",
      "1580 0.68776894 [0.8201294] [0.51312584] [0.6817578] [-0.3806092]\n",
      "1600 0.6830497 [0.8218756] [0.5116268] [0.68151087] [-0.38050172]\n",
      "1620 0.67846304 [0.8236023] [0.5101554] [0.68125564] [-0.38039383]\n",
      "1640 0.6740025 [0.82530963] [0.5087115] [0.6809925] [-0.38028553]\n",
      "1660 0.6696584 [0.82699764] [0.5072952] [0.6807212] [-0.38017705]\n",
      "1680 0.6654485 [0.82866603] [0.50590646] [0.680442] [-0.38006806]\n",
      "1700 0.66135246 [0.8303148] [0.50454545] [0.6801548] [-0.37995896]\n",
      "1720 0.6573675 [0.83194405] [0.503212] [0.67985994] [-0.3798493]\n",
      "1740 0.6535074 [0.83355355] [0.501906] [0.67955744] [-0.37973964]\n",
      "1760 0.64976066 [0.8351432] [0.50062746] [0.6792473] [-0.37962943]\n",
      "1780 0.6461165 [0.83671314] [0.49937636] [0.67892957] [-0.37951916]\n",
      "1800 0.64258957 [0.8382635] [0.49815246] [0.67860454] [-0.37940848]\n",
      "1820 0.6391638 [0.83979404] [0.49695548] [0.6782722] [-0.3792976]\n",
      "1840 0.6358427 [0.84130496] [0.49578553] [0.6779326] [-0.37918666]\n",
      "1860 0.63263 [0.8427964] [0.4946424] [0.6775862] [-0.37907526]\n",
      "1880 0.62951076 [0.84426844] [0.4935258] [0.6772328] [-0.3789638]\n",
      "1900 0.6264887 [0.8457208] [0.4924354] [0.67687243] [-0.37885225]\n",
      "1920 0.62355953 [0.8471539] [0.4913712] [0.6765053] [-0.37874022]\n",
      "1940 0.6207224 [0.8485678] [0.49033314] [0.6761319] [-0.37862816]\n",
      "1960 0.6179789 [0.8499625] [0.4893205] [0.6757517] [-0.3785161]\n",
      "1980 0.6153131 [0.851338] [0.4883331] [0.67536485] [-0.37840387]\n",
      "2000 0.61274207 [0.8526947] [0.4873709] [0.6749718] [-0.37829152]\n",
      "당신의 점수는 [159.50545]\n"
     ]
    }
   ],
   "source": [
    "x1_data = [73, 93, 89, 96, 73]\n",
    "x2_data = [80, 88, 91, 98, 66]\n",
    "x3_data = [75, 93, 90, 100, 70]\n",
    "y_data = [152, 185, 180, 196, 142]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w1 = tf.Variable(tf.random_normal([1], seed=0), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1], seed=0), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1], seed=0), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n",
    "\n",
    "hp = w1*x1 + w2*x2 + w3*x3 + b\n",
    "cost = tf.sqrt(tf.reduce_mean(tf.square(hp - y)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000057)\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001): # 학습\n",
    "    cost_v, w1_v, w2_v, w3_v, b_v, _ = sess.run([cost, w1, w2, w3, b, train], \n",
    "                                                feed_dict={x1:x1_data, x2:x2_data, x3:x3_data, y:y_data})\n",
    "    if step%20==0: print(step, cost_v, w1_v, w2_v, w3_v, b_v)\n",
    "\n",
    "print('당신의 점수는', sess.run(hp, feed_dict={x1:100, x2:70, x3:60}))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152.18616]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hp, feed_dict={x1:73, x2:80, x3:75}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 193.74454 [[-0.3097001]\n",
      " [ 2.1901624]\n",
      " [ 0.260331 ]] [0.66799265]\n",
      "100 51.972816 [[-0.19460145]\n",
      " [ 1.9621866 ]\n",
      " [ 0.2601262 ]] [0.6688607]\n",
      "200 38.762398 [[-0.06488644]\n",
      " [ 1.7881777 ]\n",
      " [ 0.2879746 ]] [0.66997385]\n",
      "300 30.363947 [[0.05390487]\n",
      " [1.6380998 ]\n",
      " [0.31651658]] [0.6710266]\n",
      "400 23.822733 [[0.15965773]\n",
      " [1.5058386 ]\n",
      " [0.3422654 ]] [0.67198676]\n",
      "500 18.698557 [[0.25339252]\n",
      " [1.3888712 ]\n",
      " [0.36506066]] [0.6728592]\n",
      "600 14.683886 [[0.33642268]\n",
      " [1.2853746 ]\n",
      " [0.38517526]] [0.67365307]\n",
      "700 11.538401 [[0.4099667 ]\n",
      " [1.1937906 ]\n",
      " [0.40290737]] [0.674377]\n",
      "800 9.073927 [[0.47511354]\n",
      " [1.1127506 ]\n",
      " [0.41853112]] [0.6750393]\n",
      "900 7.143027 [[0.53282547]\n",
      " [1.0410426 ]\n",
      " [0.4322882 ]] [0.67564684]\n",
      "1000 5.630143 [[0.5839573 ]\n",
      " [0.9775942 ]\n",
      " [0.44439456]] [0.6762056]\n",
      "1100 4.4448595 [[0.6292626 ]\n",
      " [0.92145675]\n",
      " [0.45504   ]] [0.67672133]\n",
      "1200 3.516133 [[0.66941154]\n",
      " [0.87179047]\n",
      " [0.46439356]] [0.6771987]\n",
      "1300 2.788488 [[0.70499533]\n",
      " [0.8278518 ]\n",
      " [0.4726045 ]] [0.6776422]\n",
      "1400 2.2183394 [[0.736538  ]\n",
      " [0.78898257]\n",
      " [0.47980443]] [0.6780554]\n",
      "1500 1.7716129 [[0.7645021 ]\n",
      " [0.7545996 ]\n",
      " [0.48610967]] [0.678442]\n",
      "1600 1.4215803 [[0.7892999 ]\n",
      " [0.72418886]\n",
      " [0.4916253 ]] [0.6788045]\n",
      "1700 1.1473259 [[0.8112935 ]\n",
      " [0.6972927 ]\n",
      " [0.49644145]] [0.6791461]\n",
      "1800 0.9324182 [[0.83080465]\n",
      " [0.67350805]\n",
      " [0.50063986]] [0.67946875]\n",
      "1900 0.7640094 [[0.84811693]\n",
      " [0.65247566]\n",
      " [0.504291  ]] [0.67977446]\n",
      "2000 0.6320362 [[0.8634844]\n",
      " [0.6338811]\n",
      " [0.5074602]] [0.6800656]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90],[96, 98, 100], [73, 66, 70]]\n",
    "y_data = [[152],[185],[180],[196],[142]]\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3]) # 행은 미정, 열은 3개 열의 데이터가 들어온다\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1]) # 행은 미정, 열은 1개 열의 데이터가 들어온다\n",
    "w = tf.Variable(tf.random_normal([3,1], seed=0), name='weight') # x의 열 갯수와 w의 행 갯수가 일치해야 행렬곱 가능\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "hp = tf.matmul(x,w)+b\n",
    "cost = tf.reduce_mean(tf.square(hp-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000045)\n",
    "train = optimizer.minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, weight_val, bias_val,_ = sess.run([cost, w, b, train], feed_dict={x:x_data, y:y_data})\n",
    "    if step%100==0: print(step, cost_val, weight_val, bias_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 219.40659 [[-0.30156353]\n",
      " [ 2.1982799 ]\n",
      " [ 0.26854435]] [-0.39802647]\n",
      "100 52.75884 [[-0.18898152]\n",
      " [ 1.9667323 ]\n",
      " [ 0.264032  ]] [-0.3970555]\n",
      "200 39.00582 [[-0.05885994]\n",
      " [ 1.792161  ]\n",
      " [ 0.2905756 ]] [-0.39580604]\n",
      "300 30.559803 [[0.06071639]\n",
      " [1.6419784 ]\n",
      " [0.3182377 ]] [-0.39461365]\n",
      "400 23.988739 [[0.16727607]\n",
      " [1.5097061 ]\n",
      " [0.34316784]] [-0.39351466]\n",
      "500 18.841232 [[0.26179406]\n",
      " [1.3927671 ]\n",
      " [0.36515656]] [-0.39250463]\n",
      "600 14.808004 [[0.34557787]\n",
      " [1.2893256 ]\n",
      " [0.38446867]] [-0.39157447]\n",
      "700 11.647983 [[0.4198491]\n",
      " [1.1978216]\n",
      " [0.4014028]] [-0.39071536]\n",
      "800 9.171924 [[0.48569682]\n",
      " [1.1168802 ]\n",
      " [0.41623166]] [-0.3899193]\n",
      "900 7.231898 [[0.54408926]\n",
      " [1.0452896 ]\n",
      " [0.42920035]] [-0.3891793]\n",
      "1000 5.711716 [[0.59588   ]\n",
      " [0.981974  ]\n",
      " [0.44052315]] [-0.38848916]\n",
      "1100 4.5205197 [[0.6418262 ]\n",
      " [0.92598253]\n",
      " [0.4503909 ]] [-0.38784337]\n",
      "1200 3.5871062 [[0.68259764]\n",
      " [0.8764727 ]\n",
      " [0.4589718 ]] [-0.38723695]\n",
      "1300 2.8556376 [[0.71878874]\n",
      " [0.8327013 ]\n",
      " [0.4664169 ]] [-0.3866657]\n",
      "1400 2.282394 [[0.75092286]\n",
      " [0.79400676]\n",
      " [0.47285646]] [-0.38612568]\n",
      "1500 1.8331451 [[0.77946633]\n",
      " [0.7598067 ]\n",
      " [0.47840902]] [-0.38561365]\n",
      "1600 1.4810194 [[0.80482996]\n",
      " [0.7295833 ]\n",
      " [0.48317745]] [-0.38512653]\n",
      "1700 1.205011 [[0.82737744]\n",
      " [0.7028798 ]\n",
      " [0.48725322]] [-0.38466159]\n",
      "1800 0.98860663 [[0.847432  ]\n",
      " [0.67929196]\n",
      " [0.49071833]] [-0.38421652]\n",
      "1900 0.8189514 [[0.8652778 ]\n",
      " [0.65846044]\n",
      " [0.4936432 ]] [-0.38378927]\n",
      "2000 0.6858779 [[0.8811685]\n",
      " [0.6400689]\n",
      " [0.4960927]] [-0.38337788]\n"
     ]
    }
   ],
   "source": [
    "# 파일에서 데이터 읽어오기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "xy = np.loadtxt('data/ml_test.csv', delimiter=',', dtype=np.float32) # utf-8은 인식못함\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, -1:]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w = tf.Variable(tf.random_normal([3,1], seed=0), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n",
    "\n",
    "hp = tf.matmul(x,w)+b\n",
    "cost = tf.reduce_mean(tf.square(hp-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000045)\n",
    "train = optimizer.minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, weight_val, bias_val,_ = sess.run([cost, w, b, train], feed_dict={x:x_data, y:y_data})\n",
    "    if step%100==0: print(step, cost_val, weight_val, bias_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feed forward\n",
    "입력 --------> 출력\n",
    "         |\n",
    "       bias\n",
    "\n",
    "$y = wx + b$\n",
    "$w = 2$\n",
    "$b = 1$\n",
    "\n",
    "x   y  \n",
    "0   1  \n",
    "1   3  \n",
    "2   5  \n",
    "  \n",
    "목표 = 입력1 -> 출력4\n",
    "$b = 1$  \n",
    "w   y  \n",
    "1   2  \n",
    "2   3  \n",
    "3   4  \n",
    "  \n",
    "입력 1  \n",
    "weight 2  \n",
    "목표 4  \n",
    "  \n",
    "b    y   \n",
    "1    3  \n",
    "1.5  3.5  \n",
    "2    4  \n",
    "\n",
    "# backpropagation 역전파\n",
    "-  미분을 사용해 오차를 줄이는 방법\n",
    "\n",
    "-  ## cost function\n",
    "    -  신경망 학습에서 학습데이터에 대한 오차를 측정하는 척도\n",
    "    -  ### 평균제곱오차(mean squared error, MSE)\n",
    "        -  $오차 = {1\\over2}\\sum(yTarget - y)^2$  \n",
    "\n",
    "-  ## gradient descendent method(경사하강법)\n",
    "    -  $w수정 = w - \\alpha * {\\partial E \\over \\partial w}$ ($\\alpha$: learning rate)\n",
    "\n",
    "$f = wx+b$  \n",
    "$g = wx$  \n",
    "$f = g+b$  \n",
    "\n",
    "${\\partial g \\over \\partial w} = x$  \n",
    "${\\partial g \\over \\partial x} = w$  \n",
    "${\\partial f \\over \\partial g} = 1$  \n",
    "${\\partial g \\over \\partial b} = 1$  \n",
    "\n",
    "${\\partial f \\over \\partial w} = {\\partial f \\over \\partial g} * {\\partial g \\over \\partial w}$  \n",
    "${\\partial f \\over \\partial x} = {\\partial f \\over \\partial g} * {\\partial g \\over \\partial x}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06435000000000002"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "y = np.array([0.1, 0.03, 0.05, 0.2, 0.9, 0.0, 0.1, 0.2, 0.12, 0.03])\n",
    "sum((t-y)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차는 0.06435000000000002\n"
     ]
    }
   ],
   "source": [
    "def mse(t,y):\n",
    "    return 0.5*np.sum((t-y)**2)\n",
    "\n",
    "print('오차는', mse(t, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression\n",
    "입력(x)    출력(y)\n",
    "1          2\n",
    "2          4\n",
    "3          6\n",
    "4          8\n",
    "5          10\n",
    "6          12\n",
    "\n",
    "7을 입력하면 출력값은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_data = [1,2,3,4,5,6]\n",
    "y_data = [2,4,6,8,10,12]\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w = tf.Variable(tf.random_normal([1], seed=0), name='weight') #1자리 난수, seed 고정\n",
    "b = tf.Variable(tf.random_normal([1], seed=0), name='bias') \n",
    "\n",
    "hypothesis = w*x+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y)) \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 94.1615 [-0.32358906] [-0.38156518]\n",
      "20 25.674927 [0.76995075] [-0.12730911]\n",
      "40 7.0030427 [1.3410589] [0.00492619]\n",
      "60 1.9123893 [1.639381] [0.07345283]\n",
      "80 0.5244544 [1.7952688] [0.10871811]\n",
      "100 0.14600922 [1.8767844] [0.12661985]\n",
      "120 0.042788524 [1.9194659] [0.1354591]\n",
      "140 0.014603357 [1.9418697] [0.13957007]\n",
      "160 0.006876567 [1.9536849] [0.14121583]\n",
      "180 0.0047279648 [1.9599702] [0.14157806]\n",
      "200 0.004100854 [1.9633672] [0.14127372]\n",
      "220 0.003889154 [1.9652554] [0.14062494]\n",
      "240 0.0037912459 [1.9663548] [0.13979986]\n",
      "260 0.0037249755 [1.9670416] [0.1388863]\n",
      "280 0.003667903 [1.9675124] [0.13793]\n",
      "300 0.0036138955 [1.9678692] [0.1369549]\n",
      "320 0.0035612788 [1.9681656] [0.13597347]\n",
      "340 0.0035095997 [1.96843] [0.13499214]\n",
      "360 0.003458739 [1.9686769] [0.13401428]\n",
      "380 0.003408597 [1.9689137] [0.13304168]\n",
      "400 0.0033591967 [1.9691445] [0.1320751]\n",
      "420 0.0033104979 [1.9693714] [0.13111502]\n",
      "440 0.0032625205 [1.9695954] [0.13016166]\n",
      "460 0.003215242 [1.9698172] [0.1292151]\n",
      "480 0.0031686304 [1.9700369] [0.12827533]\n",
      "500 0.003122696 [1.9702549] [0.12734236]\n",
      "520 0.0030774483 [1.9704716] [0.12641618]\n",
      "540 0.0030328443 [1.9706862] [0.12549672]\n",
      "560 0.0029888863 [1.9708993] [0.12458394]\n",
      "580 0.0029455458 [1.9711114] [0.12367781]\n",
      "600 0.0029028738 [1.9713212] [0.12277825]\n",
      "620 0.0028607948 [1.9715297] [0.12188523]\n",
      "640 0.0028193214 [1.9717371] [0.12099873]\n",
      "660 0.0027784656 [1.9719423] [0.12011866]\n",
      "680 0.0027382092 [1.9721465] [0.119245]\n",
      "700 0.0026985223 [1.9723492] [0.11837772]\n",
      "720 0.0026594002 [1.97255] [0.11751672]\n",
      "740 0.0026208402 [1.9727501] [0.11666197]\n",
      "760 0.0025828579 [1.972948] [0.11581345]\n",
      "780 0.002545437 [1.9731448] [0.11497111]\n",
      "800 0.0025085427 [1.9733403] [0.11413491]\n",
      "820 0.00247219 [1.9735339] [0.11330476]\n",
      "840 0.0024363417 [1.9737267] [0.11248069]\n",
      "860 0.0024010583 [1.9739175] [0.11166261]\n",
      "880 0.0023662385 [1.9741073] [0.11085048]\n",
      "900 0.0023319416 [1.9742956] [0.11004428]\n",
      "920 0.002298158 [1.9744824] [0.10924392]\n",
      "940 0.0022648342 [1.9746684] [0.10844935]\n",
      "960 0.0022320126 [1.9748522] [0.10766058]\n",
      "980 0.0021996726 [1.9750355] [0.10687754]\n",
      "1000 0.0021677862 [1.9752167] [0.10610022]\n",
      "1020 0.002136379 [1.9753971] [0.10532851]\n",
      "1040 0.0021053955 [1.9755759] [0.10456245]\n",
      "1060 0.002074884 [1.9757537] [0.10380194]\n",
      "1080 0.0020448265 [1.9759301] [0.10304698]\n",
      "1100 0.0020151772 [1.976105] [0.10229748]\n",
      "1120 0.0019859802 [1.976279] [0.10155346]\n",
      "1140 0.0019571895 [1.9764513] [0.10081483]\n",
      "1160 0.001928829 [1.9766229] [0.10008159]\n",
      "1180 0.0019008731 [1.9767925] [0.09935368]\n",
      "1200 0.0018733245 [1.9769617] [0.09863108]\n",
      "1220 0.0018461713 [1.9771289] [0.09791372]\n",
      "1240 0.0018194104 [1.9772958] [0.09720157]\n",
      "1260 0.001793045 [1.9774604] [0.09649459]\n",
      "1280 0.0017670464 [1.9776249] [0.09579277]\n",
      "1300 0.0017414433 [1.977787] [0.09509605]\n",
      "1320 0.0017162076 [1.9779491] [0.09440441]\n",
      "1340 0.0016913345 [1.9781089] [0.09371781]\n",
      "1360 0.00166682 [1.9782686] [0.09303619]\n",
      "1380 0.0016426701 [1.9784262] [0.09235952]\n",
      "1400 0.0016188439 [1.9785836] [0.09168778]\n",
      "1420 0.0015953918 [1.9787389] [0.0910209]\n",
      "1440 0.0015722715 [1.9788939] [0.09035891]\n",
      "1460 0.0015494839 [1.979047] [0.0897017]\n",
      "1480 0.0015270271 [1.9791995] [0.08904932]\n",
      "1500 0.0015048888 [1.9793507] [0.08840165]\n",
      "1520 0.0014830902 [1.9795009] [0.08775873]\n",
      "1540 0.0014615818 [1.9796501] [0.08712044]\n",
      "1560 0.0014404139 [1.979798] [0.08648683]\n",
      "1580 0.0014195368 [1.9799452] [0.08585779]\n",
      "1600 0.0013989694 [1.9800906] [0.08523336]\n",
      "1620 0.0013786862 [1.980236] [0.08461347]\n",
      "1640 0.0013586972 [1.9803791] [0.08399808]\n",
      "1660 0.0013390034 [1.9805222] [0.0833872]\n",
      "1680 0.0013196045 [1.9806635] [0.08278071]\n",
      "1700 0.0013004867 [1.9808042] [0.08217868]\n",
      "1720 0.0012816308 [1.980944] [0.08158097]\n",
      "1740 0.0012630472 [1.9810823] [0.08098766]\n",
      "1760 0.0012447421 [1.9812205] [0.08039864]\n",
      "1780 0.0012267207 [1.9813565] [0.0798139]\n",
      "1800 0.0012089205 [1.9814924] [0.07923344]\n",
      "1820 0.0011914152 [1.9816269] [0.07865714]\n",
      "1840 0.0011741513 [1.9817604] [0.07808509]\n",
      "1860 0.0011571272 [1.9818935] [0.07751717]\n",
      "1880 0.001140352 [1.9820247] [0.0769534]\n",
      "1900 0.0011238226 [1.9821558] [0.07639374]\n",
      "1920 0.001107541 [1.9822853] [0.0758381]\n",
      "1940 0.001091486 [1.982414] [0.07528657]\n",
      "1960 0.0010756744 [1.9825425] [0.074739]\n",
      "1980 0.0010600793 [1.9826689] [0.07419543]\n",
      "2000 0.0010447168 [1.9827952] [0.07365584]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001): # 학습\n",
    "    cost_v, w_v, b_v, _ = sess.run([cost, w, b, train], feed_dict={x:x_data, y:y_data})\n",
    "    if step%20==0: print(step, cost_v, w_v, b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.953222]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict={x:7})) # 결과예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "기울기 = 4.3\n",
    "절편 = 64\n",
    "공부시간   점수\n",
    "2          71\n",
    "4          83\n",
    "6          91\n",
    "8          97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = [4.3, 64]\n",
    "data = [[2, 71], [4, 83], [6, 91], [8, 97]]\n",
    "\n",
    "x = [time for time, score in data]\n",
    "y = [score for time, score in data]\n",
    "\n",
    "def predict(x):\n",
    "    return ab[0] * x + ab[1]\n",
    "\n",
    "predict(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공부시간: 2, 실제점수: 71, 예측점수: 72.6\n",
      "공부시간: 4, 실제점수: 83, 예측점수: 81.2\n",
      "공부시간: 6, 실제점수: 91, 예측점수: 89.8\n",
      "공부시간: 8, 실제점수: 97, 예측점수: 98.4\n",
      "오차: 1.5165750888103096\n"
     ]
    }
   ],
   "source": [
    "def rmse(p, a): # RMSE(Root Mean Squared Error) 평균 제곱 오차\n",
    "    return np.sqrt(((p-a)**2).mean())\n",
    "\n",
    "def rmse_val(predict_result, y):\n",
    "    return rmse(np.array(predict_result), np.array(y))\n",
    "\n",
    "predict_result = []\n",
    "for i in range(len(x)):\n",
    "    pred = predict(x[i])\n",
    "    predict_result.append(pred)\n",
    "    print('공부시간: {}, 실제점수: {}, 예측점수: {}'.format(x[i], y[i], pred))\n",
    "\n",
    "print('오차:', rmse_val(predict_result, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "선형회귀\n",
    "-  임의의 직선을 그어 이에 대한 평균제곱근 오차를 구하고 이 값을 가장 작게 만들어주는 기울기와 절편을 찾아가는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 88.53379 [0.12377319] [-0.2998792]\n",
      "200 22.305552 [13.427187] [9.532862]\n",
      "400 19.080303 [12.099842] [17.453886]\n",
      "600 15.862174 [10.773965] [25.36615]\n",
      "800 12.656544 [9.450672] [33.262993]\n",
      "1000 9.475954 [8.132574] [41.128838]\n",
      "1200 6.357134 [6.8274016] [48.91755]\n",
      "1400 3.4582522 [5.569645] [56.4233]\n",
      "1600 1.6936241 [4.6064615] [62.171158]\n",
      "1800 1.5194428 [4.3378873] [63.773983]\n",
      "2000 1.5173832 [4.313262] [63.975124]\n"
     ]
    }
   ],
   "source": [
    "x_data = [2,4,6,8]\n",
    "y_data = [71, 83, 91, 97]\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w = tf.Variable(tf.random_normal([1], seed=0), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n",
    "\n",
    "hp = w*x+b\n",
    "# cost = tf.reduce_mean(tf.square(hp - y))\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.015)\n",
    "cost = tf.sqrt(tf.reduce_mean(tf.square(hp - y)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "train = optimizer.minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001): # 학습\n",
    "    cost_v, w_v, b_v, _ = sess.run([cost, w, b, train], feed_dict={x:x_data, y:y_data})\n",
    "    if step%200==0: print(step, cost_v, w_v, b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.40122]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hp, feed_dict={x:8}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
